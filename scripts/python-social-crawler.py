#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ Pythonç¤¾äº¤åª’ä½“å†…å®¹è·å–å¼•æ“
é›†æˆå¤šä¸ªæˆç†Ÿåº“ï¼Œä¸ºLabubuå‚ç›´ç¤¾åŒºæä¾›æ•°æ®æ”¯æŒ
"""

import asyncio
import json
import logging
import os
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import requests
from dataclasses import dataclass, asdict
import sqlite3
import hashlib

# ä¾èµ–åº“ (éœ€è¦å®‰è£…)
try:
    import newspaper
    from newspaper import Article, build
    print("âœ… newspaper4k å·²å®‰è£…")
except ImportError:
    print("âŒ è¯·å®‰è£…: pip install newspaper4k")

try:
    import feedparser
    print("âœ… feedparser å·²å®‰è£…")
except ImportError:
    print("âŒ è¯·å®‰è£…: pip install feedparser")

try:
    import tweepy
    print("âœ… tweepy å·²å®‰è£…")
except ImportError:
    print("âŒ è¯·å®‰è£…: pip install tweepy")

try:
    from twscrape import API as TwScrapeAPI
    print("âœ… twscrape å·²å®‰è£…")
except ImportError:
    print("âŒ è¯·å®‰è£…: pip install git+https://github.com/vladkens/twscrape.git")

try:
    import praw  # Reddit API
    print("âœ… praw å·²å®‰è£…")
except ImportError:
    print("âŒ è¯·å®‰è£…: pip install praw")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('python_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class SocialContent:
    """ç¤¾äº¤åª’ä½“å†…å®¹æ•°æ®ç±»"""
    id: str
    title: str
    content: str
    summary: str
    author: str
    platform: str
    url: str
    published_at: datetime
    language: str
    country: str
    image_urls: List[str]
    tags: List[str]
    category: str
    engagement: Dict[str, int]  # likes, shares, comments, views
    raw_data: Dict[str, Any]

class PythonSocialCrawler:
    """Pythonç¤¾äº¤åª’ä½“å†…å®¹çˆ¬è™«å¼•æ“"""
    
    def __init__(self, config_path: str = None):
        self.config = self._load_config(config_path)
        self.db_path = 'social_crawler.db'
        self._init_database()
        
        # Labubuç›¸å…³å…³é”®è¯
        self.labubu_keywords = [
            'labubu', 'lĞ°bubu', 'æ‹‰å¸ƒå¸ƒ', 'æ³¡æ³¡ç›ç‰¹', 'popmart', 'pop mart',
            'lisa', 'blackpink', 'ç›²ç›’', 'blind box', 'æ‰‹åŠ', 'figure',
            'collectible', 'designer toy', 'æ”¶è—', 'é™é‡', 'limited edition',
            'kaws', 'molly', 'dimoo', 'skullpanda', 'hirono'
        ]
    
    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            "twitter": {
                "bearer_token": os.getenv('TWITTER_BEARER_TOKEN'),
                "consumer_key": os.getenv('TWITTER_CONSUMER_KEY'),
                "consumer_secret": os.getenv('TWITTER_CONSUMER_SECRET'),
                "access_token": os.getenv('TWITTER_ACCESS_TOKEN'),
                "access_token_secret": os.getenv('TWITTER_ACCESS_TOKEN_SECRET'),
            },
            "reddit": {
                "client_id": os.getenv('REDDIT_CLIENT_ID'),
                "client_secret": os.getenv('REDDIT_CLIENT_SECRET'),
                "user_agent": "LabubuCrawler/1.0"
            },
            "news_sources": [
                "https://hypebeast.com/feed",
                "https://www.toynews-online.biz/feed/",
                "https://www.popculthq.com/feed/",
                "https://www.kidscreen.com/feed/"
            ]
        }
        
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                default_config.update(user_config)
        
        return default_config
    
    def _init_database(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS social_content (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                content TEXT,
                summary TEXT,
                author TEXT,
                platform TEXT NOT NULL,
                url TEXT UNIQUE,
                published_at TIMESTAMP,
                language TEXT,
                country TEXT,
                image_urls TEXT,  -- JSON array
                tags TEXT,        -- JSON array
                category TEXT,
                engagement TEXT,  -- JSON object
                raw_data TEXT,    -- JSON object
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                hot_score REAL DEFAULT 0
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_platform ON social_content(platform)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_published_at ON social_content(published_at)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_hot_score ON social_content(hot_score)')
        
        conn.commit()
        conn.close()
        logger.info("ğŸ“Š æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    
    def _is_labubu_related(self, text: str) -> bool:
        """æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸Labubuç›¸å…³"""
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in self.labubu_keywords)
    
    def _calculate_hot_score(self, content: SocialContent) -> float:
        """è®¡ç®—å†…å®¹çƒ­åº¦åˆ†æ•°"""
        base_score = 0
        
        # äº’åŠ¨æ•°æ®æƒé‡
        engagement = content.engagement
        likes = engagement.get('likes', 0)
        shares = engagement.get('shares', 0)
        comments = engagement.get('comments', 0)
        views = engagement.get('views', 0)
        
        # åŸºç¡€äº’åŠ¨åˆ†æ•°
        base_score += likes * 1.0
        base_score += shares * 2.0  # åˆ†äº«æƒé‡æ›´é«˜
        base_score += comments * 1.5
        base_score += views * 0.01
        
        # æ—¶é—´è¡°å‡ (è¶Šæ–°è¶Šçƒ­)
        hours_old = (datetime.now() - content.published_at).total_seconds() / 3600
        time_factor = max(0.1, 1 - (hours_old / 168))  # ä¸€å‘¨å†…çº¿æ€§è¡°å‡
        
        # å…³é”®è¯åŒ¹é…åŠ åˆ†
        keyword_bonus = 1.0
        content_text = (content.title + ' ' + content.content).lower()
        high_value_keywords = ['lisa', 'blackpink', 'é™é‡', 'limited edition']
        if any(kw.lower() in content_text for kw in high_value_keywords):
            keyword_bonus = 1.5
        
        final_score = base_score * time_factor * keyword_bonus
        return round(final_score, 2)
    
    async def crawl_news_feeds(self) -> List[SocialContent]:
        """çˆ¬å–æ–°é—»RSSæº"""
        logger.info("ğŸ“° å¼€å§‹çˆ¬å–æ–°é—»RSSæº...")
        results = []
        
        for feed_url in self.config['news_sources']:
            try:
                logger.info(f"ğŸ” è§£æRSS: {feed_url}")
                feed = feedparser.parse(feed_url)
                
                for entry in feed.entries[:10]:  # é™åˆ¶æ¯ä¸ªæº10æ¡
                    # æ£€æŸ¥æ˜¯å¦ä¸Labubuç›¸å…³
                    text_to_check = (entry.get('title', '') + ' ' + 
                                   entry.get('description', '') + ' ' + 
                                   entry.get('summary', ''))
                    
                    if not self._is_labubu_related(text_to_check):
                        continue
                    
                    # ä½¿ç”¨newspaper4kæå–å®Œæ•´æ–‡ç« 
                    try:
                        article = Article(entry.link, language='en')
                        article.download()
                        article.parse()
                        article.nlp()
                        
                        content = SocialContent(
                            id=hashlib.md5(entry.link.encode()).hexdigest(),
                            title=entry.title,
                            content=article.text,
                            summary=article.summary,
                            author=', '.join(article.authors) if article.authors else 'Unknown',
                            platform='news_rss',
                            url=entry.link,
                            published_at=datetime(*entry.published_parsed[:6]) if hasattr(entry, 'published_parsed') else datetime.now(),
                            language='en',
                            country='US',
                            image_urls=[article.top_image] if article.top_image else [],
                            tags=article.keywords[:10],  # é™åˆ¶10ä¸ªå…³é”®è¯
                            category='News',
                            engagement={'likes': 0, 'shares': 0, 'comments': 0, 'views': 0},
                            raw_data=dict(entry)
                        )
                        
                        results.append(content)
                        logger.info(f"âœ… æˆåŠŸæå–æ–‡ç« : {content.title[:50]}...")
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ æ–‡ç« æå–å¤±è´¥ {entry.link}: {e}")
                        continue
                        
                await asyncio.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                
            except Exception as e:
                logger.error(f"âŒ RSSæºè§£æå¤±è´¥ {feed_url}: {e}")
                continue
        
        logger.info(f"ğŸ“° æ–°é—»RSSçˆ¬å–å®Œæˆï¼Œè·å¾— {len(results)} æ¡ç›¸å…³å†…å®¹")
        return results
    
    async def crawl_twitter_data(self) -> List[SocialContent]:
        """çˆ¬å–Twitteræ•°æ®"""
        logger.info("ğŸ¦ å¼€å§‹çˆ¬å–Twitteræ•°æ®...")
        results = []
        
        try:
            # æ–¹æ³•1: ä½¿ç”¨å®˜æ–¹API (å¦‚æœæœ‰token)
            if self.config['twitter']['bearer_token']:
                auth = tweepy.Client(bearer_token=self.config['twitter']['bearer_token'])
                
                # æœç´¢ç›¸å…³æ¨æ–‡
                query = ' OR '.join(self.labubu_keywords[:5])  # é™åˆ¶æŸ¥è¯¢é•¿åº¦
                tweets = auth.search_recent_tweets(
                    query=query,
                    max_results=50,
                    tweet_fields=['author_id', 'created_at', 'public_metrics', 'lang']
                )
                
                if tweets.data:
                    for tweet in tweets.data:
                        content = SocialContent(
                            id=f"twitter_{tweet.id}",
                            title=tweet.text[:100] + '...' if len(tweet.text) > 100 else tweet.text,
                            content=tweet.text,
                            summary=tweet.text[:200] + '...' if len(tweet.text) > 200 else tweet.text,
                            author=f"user_{tweet.author_id}",
                            platform='twitter',
                            url=f"https://twitter.com/user/status/{tweet.id}",
                            published_at=tweet.created_at,
                            language=tweet.lang,
                            country='Unknown',
                            image_urls=[],
                            tags=self._extract_hashtags(tweet.text),
                            category='Social',
                            engagement={
                                'likes': tweet.public_metrics['like_count'],
                                'shares': tweet.public_metrics['retweet_count'],
                                'comments': tweet.public_metrics['reply_count'],
                                'views': tweet.public_metrics.get('impression_count', 0)
                            },
                            raw_data={'tweet_id': tweet.id, 'author_id': tweet.author_id}
                        )
                        results.append(content)
            
            # æ–¹æ³•2: ä½¿ç”¨twscrape (å¦‚æœé…ç½®äº†è´¦æˆ·)
            elif os.path.exists('twscrape_accounts.txt'):
                try:
                    api = TwScrapeAPI()
                    query = f"{' OR '.join(self.labubu_keywords[:3])} -is:retweet"
                    
                    async for tweet in api.search(query, limit=30):
                        content = SocialContent(
                            id=f"twitter_scrape_{tweet.id}",
                            title=tweet.rawContent[:100] + '...' if len(tweet.rawContent) > 100 else tweet.rawContent,
                            content=tweet.rawContent,
                            summary=tweet.rawContent[:200] + '...' if len(tweet.rawContent) > 200 else tweet.rawContent,
                            author=tweet.user.username,
                            platform='twitter_scrape',
                            url=tweet.url,
                            published_at=tweet.date,
                            language=tweet.lang,
                            country='Unknown',
                            image_urls=[],
                            tags=self._extract_hashtags(tweet.rawContent),
                            category='Social',
                            engagement={
                                'likes': tweet.likeCount,
                                'shares': tweet.retweetCount,
                                'comments': tweet.replyCount,
                                'views': tweet.viewCount if hasattr(tweet, 'viewCount') else 0
                            },
                            raw_data={'tweet_id': tweet.id, 'user_id': tweet.user.id}
                        )
                        results.append(content)
                except Exception as e:
                    logger.warning(f"âš ï¸ twscrapeè·å–å¤±è´¥: {e}")
            
        except Exception as e:
            logger.error(f"âŒ Twitteræ•°æ®è·å–å¤±è´¥: {e}")
        
        logger.info(f"ğŸ¦ Twitterçˆ¬å–å®Œæˆï¼Œè·å¾— {len(results)} æ¡å†…å®¹")
        return results
    
    def _extract_hashtags(self, text: str) -> List[str]:
        """æå–hashtags"""
        import re
        hashtags = re.findall(r'#(\w+)', text)
        return hashtags[:10]  # é™åˆ¶10ä¸ª
    
    async def crawl_reddit_data(self) -> List[SocialContent]:
        """çˆ¬å–Redditæ•°æ®"""
        logger.info("ğŸ¤– å¼€å§‹çˆ¬å–Redditæ•°æ®...")
        results = []
        
        try:
            if not all([self.config['reddit']['client_id'], 
                       self.config['reddit']['client_secret']]):
                logger.warning("âš ï¸ Reddité…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡")
                return results
                
            reddit = praw.Reddit(
                client_id=self.config['reddit']['client_id'],
                client_secret=self.config['reddit']['client_secret'],
                user_agent=self.config['reddit']['user_agent']
            )
            
            # æœç´¢ç›¸å…³subredditså’Œå¸–å­
            subreddits = ['popmart', 'blindbox', 'designertoys', 'kpop', 'blackpink']
            
            for sub_name in subreddits:
                try:
                    subreddit = reddit.subreddit(sub_name)
                    
                    # è·å–çƒ­é—¨å¸–å­
                    for submission in subreddit.hot(limit=10):
                        if not self._is_labubu_related(submission.title + ' ' + submission.selftext):
                            continue
                        
                        content = SocialContent(
                            id=f"reddit_{submission.id}",
                            title=submission.title,
                            content=submission.selftext,
                            summary=submission.title,
                            author=str(submission.author) if submission.author else 'Unknown',
                            platform='reddit',
                            url=f"https://reddit.com{submission.permalink}",
                            published_at=datetime.fromtimestamp(submission.created_utc),
                            language='en',
                            country='Global',
                            image_urls=[submission.url] if submission.url.endswith(('.jpg', '.png', '.gif')) else [],
                            tags=[sub_name],
                            category='Discussion',
                            engagement={
                                'likes': submission.score,
                                'shares': 0,
                                'comments': submission.num_comments,
                                'views': 0
                            },
                            raw_data={'subreddit': sub_name, 'submission_id': submission.id}
                        )
                        results.append(content)
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ Subreddit {sub_name} è·å–å¤±è´¥: {e}")
                    continue
                    
                await asyncio.sleep(2)  # Redditæœ‰ä¸¥æ ¼çš„é€Ÿç‡é™åˆ¶
                
        except Exception as e:
            logger.error(f"âŒ Redditæ•°æ®è·å–å¤±è´¥: {e}")
        
        logger.info(f"ğŸ¤– Redditçˆ¬å–å®Œæˆï¼Œè·å¾— {len(results)} æ¡å†…å®¹")
        return results
    
    def save_to_database(self, contents: List[SocialContent]):
        """ä¿å­˜å†…å®¹åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        saved_count = 0
        for content in contents:
            try:
                # è®¡ç®—çƒ­åº¦åˆ†æ•°
                hot_score = self._calculate_hot_score(content)
                
                cursor.execute('''
                    INSERT OR REPLACE INTO social_content 
                    (id, title, content, summary, author, platform, url, published_at, 
                     language, country, image_urls, tags, category, engagement, 
                     raw_data, hot_score)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    content.id,
                    content.title,
                    content.content,
                    content.summary,
                    content.author,
                    content.platform,
                    content.url,
                    content.published_at,
                    content.language,
                    content.country,
                    json.dumps(content.image_urls),
                    json.dumps(content.tags),
                    content.category,
                    json.dumps(content.engagement),
                    json.dumps(content.raw_data),
                    hot_score
                ))
                saved_count += 1
                
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜å†…å®¹å¤±è´¥ {content.id}: {e}")
                continue
        
        conn.commit()
        conn.close()
        logger.info(f"ğŸ’¾ æˆåŠŸä¿å­˜ {saved_count}/{len(contents)} æ¡å†…å®¹åˆ°æ•°æ®åº“")
    
    def export_to_json(self, output_file: str = None) -> str:
        """å¯¼å‡ºæ•°æ®ä¸ºJSONæ ¼å¼ï¼Œä¸ç°æœ‰Next.jsé¡¹ç›®é›†æˆ"""
        if not output_file:
            output_file = f"labubu_social_data_{int(time.time())}.json"
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # è·å–æœ€æ–°çš„çƒ­é—¨å†…å®¹
        cursor.execute('''
            SELECT * FROM social_content 
            ORDER BY hot_score DESC, published_at DESC 
            LIMIT 100
        ''')
        
        rows = cursor.fetchall()
        columns = [description[0] for description in cursor.description]
        
        # è½¬æ¢ä¸ºä¸Next.jsé¡¹ç›®å…¼å®¹çš„æ ¼å¼
        export_data = {
            "metadata": {
                "export_time": datetime.now().isoformat(),
                "total_count": len(rows),
                "source": "python_social_crawler",
                "version": "1.0"
            },
            "articles": []
        }
        
        for row in rows:
            row_dict = dict(zip(columns, row))
            
            # è½¬æ¢ä¸ºNext.jsé¡¹ç›®æœŸæœ›çš„æ ¼å¼
            article = {
                "id": row_dict['id'],
                "title": row_dict['title'],
                "content": row_dict['content'],
                "summary": row_dict['summary'],
                "author": row_dict['author'],
                "sourceId": row_dict['platform'],
                "originalUrl": row_dict['url'],
                "publishedAt": row_dict['published_at'],
                "imageUrls": json.loads(row_dict['image_urls'] or '[]'),
                "tags": json.loads(row_dict['tags'] or '[]'),
                "category": row_dict['category'],
                "language": row_dict['language'],
                "country": row_dict['country'],
                "platform": row_dict['platform'],
                "engagementData": json.loads(row_dict['engagement'] or '{}'),
                "hotScore": row_dict['hot_score'],
                "createdAt": row_dict['created_at']
            }
            export_data["articles"].append(article)
        
        conn.close()
        
        # ä¿å­˜JSONæ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ“„ æ•°æ®å¯¼å‡ºå®Œæˆ: {output_file}")
        return output_file
    
    async def run_full_crawl(self) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„çˆ¬å–ä»»åŠ¡"""
        logger.info("ğŸš€ å¼€å§‹å®Œæ•´çš„ç¤¾äº¤åª’ä½“å†…å®¹çˆ¬å–...")
        start_time = time.time()
        
        all_contents = []
        stats = {
            "news_count": 0,
            "twitter_count": 0,
            "reddit_count": 0,
            "total_count": 0,
            "duration_seconds": 0
        }
        
        try:
            # å¹¶å‘æ‰§è¡Œå¤šä¸ªçˆ¬å–ä»»åŠ¡
            tasks = [
                self.crawl_news_feeds(),
                self.crawl_twitter_data(),
                self.crawl_reddit_data()
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"âŒ ä»»åŠ¡ {i} æ‰§è¡Œå¤±è´¥: {result}")
                    continue
                
                if i == 0:  # news
                    stats["news_count"] = len(result)
                elif i == 1:  # twitter
                    stats["twitter_count"] = len(result)
                elif i == 2:  # reddit
                    stats["reddit_count"] = len(result)
                
                all_contents.extend(result)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            if all_contents:
                self.save_to_database(all_contents)
                
                # å¯¼å‡ºJSONä¾›Next.jsä½¿ç”¨
                json_file = self.export_to_json()
                
                stats["total_count"] = len(all_contents)
                stats["duration_seconds"] = round(time.time() - start_time, 2)
                stats["json_export"] = json_file
                
                logger.info(f"ğŸ‰ çˆ¬å–å®Œæˆ! æ€»è®¡ {stats['total_count']} æ¡å†…å®¹ï¼Œè€—æ—¶ {stats['duration_seconds']} ç§’")
            else:
                logger.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•å†…å®¹")
                
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            raise
        
        return stats

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Pythonç¤¾äº¤åª’ä½“å†…å®¹çˆ¬è™«å¯åŠ¨...")
    
    crawler = PythonSocialCrawler()
    
    try:
        # æ‰§è¡Œå®Œæ•´çˆ¬å–
        stats = await crawler.run_full_crawl()
        
        print("\nğŸ“Š çˆ¬å–ç»Ÿè®¡:")
        print(f"  ğŸ“° æ–°é—»å†…å®¹: {stats['news_count']} æ¡")
        print(f"  ğŸ¦ Twitter: {stats['twitter_count']} æ¡")
        print(f"  ğŸ¤– Reddit: {stats['reddit_count']} æ¡")
        print(f"  ğŸ“„ JSONå¯¼å‡º: {stats.get('json_export', 'N/A')}")
        print(f"  â±ï¸ æ€»è€—æ—¶: {stats['duration_seconds']} ç§’")
        print(f"  ğŸ¯ æ€»è®¡: {stats['total_count']} æ¡Labubuç›¸å…³å†…å®¹")
        
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    # è¿è¡Œçˆ¬è™«
    exit_code = asyncio.run(main())
    exit(exit_code) 