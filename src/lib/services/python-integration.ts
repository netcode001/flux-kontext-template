// ğŸ Pythonçˆ¬è™«ä¸Next.jsé¡¹ç›®é›†æˆæœåŠ¡
// æ¡¥æ¥Pythonæ•°æ®è·å–èƒ½åŠ›ä¸ç°æœ‰TypeScriptå†…å®¹å¼•æ“

import { createAdminClient } from '@/lib/supabase/server'
import { spawn } from 'child_process'
import { promises as fs } from 'fs'
import path from 'path'

// ğŸ”„ Pythonçˆ¬è™«æ•°æ®æ¥å£
interface PythonCrawlerData {
  metadata: {
    export_time: string
    total_count: number
    source: string
    version: string
  }
  articles: PythonCrawlerArticle[]
}

interface PythonCrawlerArticle {
  id: string
  title: string
  content: string
  summary: string
  author: string
  sourceId: string
  originalUrl: string
  publishedAt: string
  imageUrls: string[]
  tags: string[]
  category: string
  language: string
  country: string
  platform: string
  engagementData: {
    likes: number
    shares: number
    comments: number
    views: number
  }
  hotScore: number
  createdAt: string
}

// ğŸš€ Pythoné›†æˆæœåŠ¡ç±»
export class PythonIntegrationService {
  private supabase = createAdminClient()
  private pythonScriptPath = path.join(process.cwd(), 'scripts', 'python-social-crawler.py')
  private outputDir = path.join(process.cwd(), 'temp', 'python-output')

  constructor() {
    this.ensureOutputDir()
  }

  // ğŸ“ ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
  private async ensureOutputDir() {
    try {
      await fs.mkdir(this.outputDir, { recursive: true })
    } catch (error) {
      console.error('âŒ åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥:', error)
    }
  }

  // ğŸ æ‰§è¡ŒPythonçˆ¬è™«è„šæœ¬
  async executePythonCrawler(): Promise<{ success: boolean; data?: PythonCrawlerData; error?: string }> {
    return new Promise((resolve) => {
      console.log('ğŸš€ å¯åŠ¨Pythonç¤¾äº¤åª’ä½“çˆ¬è™«...')
      
      const pythonProcess = spawn('python3', [this.pythonScriptPath], {
        cwd: process.cwd(),
        env: {
          ...process.env,
          // ä¼ é€’ç¯å¢ƒå˜é‡ç»™Pythonè„šæœ¬
          TWITTER_BEARER_TOKEN: process.env.TWITTER_BEARER_TOKEN,
          REDDIT_CLIENT_ID: process.env.REDDIT_CLIENT_ID,
          REDDIT_CLIENT_SECRET: process.env.REDDIT_CLIENT_SECRET,
        }
      })

      let stdout = ''
      let stderr = ''

      pythonProcess.stdout.on('data', (data) => {
        const output = data.toString()
        stdout += output
        console.log('ğŸ Pythonè¾“å‡º:', output.trim())
      })

      pythonProcess.stderr.on('data', (data) => {
        const error = data.toString()
        stderr += error
        console.error('ğŸ Pythoné”™è¯¯:', error.trim())
      })

      pythonProcess.on('close', async (code) => {
        if (code === 0) {
          console.log('âœ… Pythonçˆ¬è™«æ‰§è¡ŒæˆåŠŸ')
          
          try {
            // æŸ¥æ‰¾æœ€æ–°çš„JSONè¾“å‡ºæ–‡ä»¶
            const files = await fs.readdir(process.cwd())
            const jsonFiles = files.filter(f => f.startsWith('labubu_social_data_') && f.endsWith('.json'))
            
            if (jsonFiles.length === 0) {
              resolve({ success: false, error: 'æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶' })
              return
            }

            // è·å–æœ€æ–°çš„æ–‡ä»¶
            const latestFile = jsonFiles.sort().pop()!
            const filePath = path.join(process.cwd(), latestFile)
            
            // è¯»å–æ•°æ®
            const jsonData = await fs.readFile(filePath, 'utf-8')
            const parsedData: PythonCrawlerData = JSON.parse(jsonData)
            
            resolve({ success: true, data: parsedData })
            
            // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            await fs.unlink(filePath).catch(() => {})
            
          } catch (error) {
            console.error('âŒ è¯»å–Pythonè¾“å‡ºå¤±è´¥:', error)
            resolve({ success: false, error: 'è¯»å–è¾“å‡ºæ–‡ä»¶å¤±è´¥' })
          }
        } else {
          console.error('âŒ Pythonçˆ¬è™«æ‰§è¡Œå¤±è´¥ï¼Œé€€å‡ºç :', code)
          resolve({ success: false, error: `Pythonè„šæœ¬é€€å‡ºç : ${code}, é”™è¯¯: ${stderr}` })
        }
      })

      // è®¾ç½®è¶…æ—¶ (30åˆ†é’Ÿ)
      setTimeout(() => {
        pythonProcess.kill()
        resolve({ success: false, error: 'æ‰§è¡Œè¶…æ—¶' })
      }, 30 * 60 * 1000)
    })
  }

  // ğŸ“ å°†Pythonæ•°æ®è½¬æ¢ä¸ºç°æœ‰æ ¼å¼å¹¶ä¿å­˜
  async integratePythonData(pythonData: PythonCrawlerData): Promise<{ 
    success: boolean; 
    count: number; 
    message: string 
  }> {
    try {
      console.log('ğŸ”„ å¼€å§‹é›†æˆPythonçˆ¬è™«æ•°æ®...')
      
      let successCount = 0
      let errorCount = 0

      for (const article of pythonData.articles) {
        try {
          // è½¬æ¢ä¸ºç°æœ‰æ•°æ®åº“æ ¼å¼
          const dbArticle = {
            id: article.id,
            title: article.title,
            content: article.content,
            summary: article.summary,
            author: article.author,
            source_id: await this.ensureSource(article.platform, article.sourceId),
            original_url: article.originalUrl,
            published_at: new Date(article.publishedAt).toISOString(),
            image_urls: article.imageUrls,
            tags: article.tags,
            category: article.category,
            language: article.language,
            country: article.country,
            platform: article.platform,
            
            // äº’åŠ¨æ•°æ®
            likes_count: article.engagementData.likes,
            shares_count: article.engagementData.shares,
            comments_count: article.engagementData.comments,
            views_count: article.engagementData.views,
            
            // çƒ­åº¦åˆ†æ•° (ä½¿ç”¨Pythonè®¡ç®—çš„ç»“æœ)
            hot_score: article.hotScore,
            
            // å…ƒæ•°æ®
            created_at: new Date().toISOString(),
            updated_at: new Date().toISOString(),
            
            // Pythonæ¥æºæ ‡è¯†
            source_type: 'python_crawler',
            raw_data: {
              python_id: article.id,
              original_platform: article.platform,
              crawl_time: pythonData.metadata.export_time
            }
          }

          // ä¿å­˜åˆ°æ•°æ®åº“ (ä½¿ç”¨upserté¿å…é‡å¤)
          const { error } = await this.supabase
            .from('labubu_posts')
            .upsert(dbArticle, { 
              onConflict: 'original_url',
              ignoreDuplicates: false 
            })

          if (error) {
            console.error('âŒ ä¿å­˜æ–‡ç« å¤±è´¥:', error)
            errorCount++
          } else {
            successCount++
          }

        } catch (articleError) {
          console.error('âŒ å¤„ç†æ–‡ç« å¤±è´¥:', articleError)
          errorCount++
        }
      }

      // æ›´æ–°è¶‹åŠ¿å…³é”®è¯
      await this.updateTrendingKeywords(pythonData.articles)

      const message = `Pythonæ•°æ®é›†æˆå®Œæˆ: æˆåŠŸ ${successCount} æ¡ï¼Œå¤±è´¥ ${errorCount} æ¡`
      console.log('âœ…', message)

      return {
        success: true,
        count: successCount,
        message
      }

    } catch (error) {
      console.error('âŒ Pythonæ•°æ®é›†æˆå¤±è´¥:', error)
      return {
        success: false,
        count: 0,
        message: `é›†æˆå¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`
      }
    }
  }

  // ğŸ·ï¸ ç¡®ä¿æ•°æ®æºå­˜åœ¨
  private async ensureSource(platform: string, sourceId: string): Promise<string> {
    try {
      // å…ˆæŸ¥æ‰¾æ˜¯å¦å­˜åœ¨
      const { data: existing } = await this.supabase
        .from('news_sources')
        .select('id')
        .eq('platform', platform)
        .eq('source_identifier', sourceId)
        .single()

      if (existing) {
        return existing.id
      }

      // åˆ›å»ºæ–°çš„æ•°æ®æº
      const { data: newSource, error } = await this.supabase
        .from('news_sources')
        .insert({
          name: `${platform} - ${sourceId}`,
          platform: platform,
          source_identifier: sourceId,
          url: this.getPlatformUrl(platform, sourceId),
          is_active: true,
          created_at: new Date().toISOString()
        })
        .select('id')
        .single()

      if (error) {
        console.error('âŒ åˆ›å»ºæ•°æ®æºå¤±è´¥:', error)
        return 'default_source_id' // ä½¿ç”¨é»˜è®¤æº
      }

      return newSource.id

    } catch (error) {
      console.error('âŒ ç¡®ä¿æ•°æ®æºå¤±è´¥:', error)
      return 'default_source_id'
    }
  }

  // ğŸ”— è·å–å¹³å°URL
  private getPlatformUrl(platform: string, sourceId: string): string {
    const urlMap: Record<string, string> = {
      'twitter': `https://twitter.com/${sourceId}`,
      'twitter_scrape': `https://twitter.com/search?q=${encodeURIComponent(sourceId)}`,
      'reddit': `https://reddit.com/r/${sourceId}`,
      'news_rss': sourceId, // RSS URLæœ¬èº«
      'instagram': `https://instagram.com/${sourceId}`,
      'tiktok': `https://tiktok.com/@${sourceId}`,
    }

    return urlMap[platform] || `https://${platform}.com/${sourceId}`
  }

  // ğŸ“ˆ æ›´æ–°è¶‹åŠ¿å…³é”®è¯
  private async updateTrendingKeywords(articles: PythonCrawlerArticle[]) {
    try {
      // æ”¶é›†æ‰€æœ‰æ ‡ç­¾
      const allTags = articles.flatMap(article => article.tags)
      const tagCounts = new Map<string, number>()

      allTags.forEach(tag => {
        tagCounts.set(tag, (tagCounts.get(tag) || 0) + 1)
      })

      // è·å–çƒ­é—¨æ ‡ç­¾ (å‡ºç°3æ¬¡ä»¥ä¸Š)
      const trendingTags = Array.from(tagCounts.entries())
        .filter(([_, count]) => count >= 3)
        .sort(([_, a], [__, b]) => b - a)
        .slice(0, 20)
        .map(([tag, count]) => ({ keyword: tag, count, source: 'python_crawler' }))

      if (trendingTags.length > 0) {
        // ä¿å­˜æˆ–æ›´æ–°è¶‹åŠ¿å…³é”®è¯
        for (const { keyword, count } of trendingTags) {
          await this.supabase
            .from('trending_keywords')
            .upsert({
              keyword,
              count,
              source: 'python_crawler',
              date: new Date().toISOString().split('T')[0],
              updated_at: new Date().toISOString()
            }, {
              onConflict: 'keyword,date',
              ignoreDuplicates: false
            })
        }

        console.log(`âœ… æ›´æ–°äº† ${trendingTags.length} ä¸ªè¶‹åŠ¿å…³é”®è¯`)
      }

    } catch (error) {
      console.error('âŒ æ›´æ–°è¶‹åŠ¿å…³é”®è¯å¤±è´¥:', error)
    }
  }

  // ğŸ”„ å®Œæ•´çš„é›†æˆæµç¨‹
  async runPythonIntegration(): Promise<{
    success: boolean
    stats: {
      crawl_duration?: number
      articles_found?: number
      articles_saved?: number
      trending_keywords?: number
    }
    message: string
    error?: string
  }> {
    const startTime = Date.now()
    
    try {
      console.log('ğŸš€ å¼€å§‹Pythonç¤¾äº¤åª’ä½“æ•°æ®é›†æˆæµç¨‹...')

      // 1. æ‰§è¡ŒPythonçˆ¬è™«
      const crawlResult = await this.executePythonCrawler()
      
      if (!crawlResult.success || !crawlResult.data) {
        return {
          success: false,
          stats: {},
          message: 'Pythonçˆ¬è™«æ‰§è¡Œå¤±è´¥',
          error: crawlResult.error
        }
      }

      // 2. é›†æˆæ•°æ®åˆ°ç°æœ‰ç³»ç»Ÿ
      const integrationResult = await this.integratePythonData(crawlResult.data)
      
      const stats = {
        crawl_duration: Math.round((Date.now() - startTime) / 1000),
        articles_found: crawlResult.data.articles.length,
        articles_saved: integrationResult.count,
        trending_keywords: 0 // è¿™é‡Œå¯ä»¥åŠ ä¸Šå®é™…æ•°é‡
      }

      return {
        success: integrationResult.success,
        stats,
        message: `Pythoné›†æˆå®Œæˆ: å‘ç° ${stats.articles_found} æ¡ï¼Œä¿å­˜ ${stats.articles_saved} æ¡ï¼Œè€—æ—¶ ${stats.crawl_duration} ç§’`
      }

    } catch (error) {
      return {
        success: false,
        stats: { crawl_duration: Math.round((Date.now() - startTime) / 1000) },
        message: 'Pythoné›†æˆæµç¨‹æ‰§è¡Œå¤±è´¥',
        error: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'
      }
    }
  }
}

// ğŸ¯ å¯¼å‡ºå•ä¾‹å®ä¾‹
export const pythonIntegration = new PythonIntegrationService()

// ğŸ”§ è¾…åŠ©å‡½æ•°: æ£€æŸ¥Pythonç¯å¢ƒ
export async function checkPythonEnvironment(): Promise<{
  python_available: boolean
  packages_status: Record<string, boolean>
  recommendations: string[]
}> {
  return new Promise((resolve) => {
    const pythonProcess = spawn('python3', ['-c', `
import sys
import importlib

packages = [
    'newspaper', 'feedparser', 'tweepy', 'praw', 
    'requests', 'beautifulsoup4', 'pandas'
]

results = {}
for pkg in packages:
    try:
        importlib.import_module(pkg)
        results[pkg] = True
    except ImportError:
        results[pkg] = False

print('PYTHON_CHECK_RESULT:', results)
    `])

    let output = ''
    pythonProcess.stdout.on('data', (data) => {
      output += data.toString()
    })

    pythonProcess.on('close', (code) => {
      if (code !== 0) {
        resolve({
          python_available: false,
          packages_status: {},
          recommendations: ['è¯·ç¡®ä¿å·²å®‰è£…Python 3.8+', 'è¿è¡Œ: pip install -r scripts/python-requirements.txt']
        })
        return
      }

      try {
        const resultLine = output.split('\n').find(line => line.includes('PYTHON_CHECK_RESULT:'))
        if (resultLine) {
          const packages_status = eval(resultLine.split('PYTHON_CHECK_RESULT:')[1].trim())
          const missing = Object.entries(packages_status).filter(([_, installed]) => !installed).map(([pkg]) => pkg)
          
          resolve({
            python_available: true,
            packages_status,
            recommendations: missing.length > 0 ? [
              `ç¼ºå°‘åŒ…: ${missing.join(', ')}`,
              'è¿è¡Œ: pip install -r scripts/python-requirements.txt'
            ] : []
          })
        } else {
          resolve({
            python_available: true,
            packages_status: {},
            recommendations: ['æ— æ³•æ£€æµ‹åŒ…çŠ¶æ€']
          })
        }
      } catch (error) {
        resolve({
          python_available: true,
          packages_status: {},
          recommendations: ['åŒ…çŠ¶æ€æ£€æµ‹å¤±è´¥']
        })
      }
    })
  })
} 